---
layout: post
title: Blog Post 6 -  Fake News Classification Using TensorFlow
---

With the development of the Internet, the news media has become faster and faster to spread. However, some fake news reports have also appeared in the Internet along with it. Some fake news media hope to arouse public opinion through fake news, so as to increase their own influence in the internet. Hence, it is important for everyone to have the ability to recognize the real and fake news.

In this blog post, we are going to develop a fake news classifier using Tensorflow from Python. It is highly recommended to work on this blog post in Google Colab.


## Data Source

Our data for this blog post comes from the following source:

- *Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).*

We can access it from [Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset). The data has been done a small amount of data cleaning, and performed a train-test split by Professor Phil Chodrow.

Now, let's get start!

## §1. Acquire Training Data

The following URL is the training data set. We can either read it into our Jupyter Notebook directly or download it to our computer and read it from disk. In this blog, we will use `pd.read_csv()` to read it into Python directly.


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```

Before we read the data, we need to import required packages.


```python
import pandas as pd
import numpy as np
import tensorflow as tf
import string
import re
```


```python
# Read the data set
df = pd.read_csv(train_url)
# Let's take a quick look
df.head()
```





  <div id="df-1715cec3-a27b-4cd8-b4d4-0489f15898c0">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-1715cec3-a27b-4cd8-b4d4-0489f15898c0')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }
    
    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }
    
    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }
    
    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-1715cec3-a27b-4cd8-b4d4-0489f15898c0 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';
    
        async function convertToInteractive(key) {
          const element = document.querySelector('#df-1715cec3-a27b-4cd8-b4d4-0489f15898c0');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;
    
          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




From the above pandas dataframe, we can see that each row of the data corresponds to an news article. The `title` column is the title of the article, and the `text` column is the full article text. The last column `fake` is `0` if the article is true and `1` if the article contains fake news.

## §2. Make a Dataset

Now, we are going to write a function call `make_dataset`. This function will do following two things:

1. Remove the *stopwords* from the article `text` and `title`. Note that a stopword means a word that is usually considered to be uninformative. For exampl, stopwords could be: "the", "but", "and", "or". We can get help from this [StackOverFlow thread](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe).

2. Our function should construct and return a `tf.data.Dataset`, and the function should contains two inputs and one output. Input should be of the form `(title, text)`. The output should only contains the `fake` column. 

**Write The Function**

First, we import stopwords from `nltk.corpus`


```python
# Import stopwords
import nltk
from nltk.corpus import stopwords
# Download stopwords
nltk.download('stopwords')
# Extract stopwords
stop = stopwords.words('english')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



```python
def make_dataset(df):
  # remove stopwords from texts and articles
  df['text_without_stop'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['title_without_stop'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  # contruct the dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
          {
              "title" : df[["title_without_stop"]],
              "text" : df[["text_without_stop"]]
          },
          {
              "fake" : df[["fake"]]
          }
      )
  )
  # we shuffle the dataset
  data = data.shuffle(buffer_size = len(data))

  # batch our dataset to increase the training speed
  data = data.batch(100)

  return data
```

Next, we call our function `make_dataset()` on our training dataframe to produce a Dataset. I've included `data = data.batch(100)` prior to returning our dataset. Batching causes our model to train on chunks of data rather than individual rows. Although sometimes batching can reducen the accuracy, but it can also increase the speed of training. It is important to find a balance. I found that batches of 100 rows worked well.


```python
data = make_dataset(df)
```

**Validation Data**

After we have constucted our primary `Dataset`, we split 20% of it to use for validation.


```python
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))

train_dataset = data.take(train_size)
val_dataset = data.take(val_size)
# Checking the size of training and validation set
len(train_dataset), len(val_dataset)
```




    (180, 45)



**Base Rate**

We know that the base rate refers to the accuracy of a model that always makes the same guess. Now, we need to determine the base rate for this data set by checking the labels on our training data set.


```python
a = list(data.as_numpy_iterator())[0]
labels = a[1].values()
print(sum(list(labels)[0] == 0))
print(sum(list(labels)[0] == 1))
# Compute the Base Rate
47 / 100
```

    [44]
    [56]





    0.47



We can see that the base rate is 47%, which is close to the 50%.

## §3. Create Models

Now, we are going to create our TensorFlow model to give a perspective for the following question:

> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

In order to answer the above question, we will create three TensorFlow models.

1. First model, we use only the article title as input.

2. Second model, we use only the article text as input.

3. Third model, we use both the article title and the article text as input.

Let's train our model until they appear to be fully trained. And we will compare their performance! We will need to use the Function API, rather than the Sequential API for this modeling task.

**Standardization and Vectorization**

First, we want to convert all text to lowercase and remove punctuation. Then, we want to vectorize the text by representing text as a vector (array, tensor).


```python
import string
import re
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras import Input, layers, Model, utils, losses

size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train_dataset.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train_dataset.map(lambda x, y: x["text"]))
```

**Construct Model**

Now, we've successfully prepared our data, and it's time to construct our models! The first step is to specifiy the two kinds of `keras.Input` for our model.


```python
title_input = Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

**Hidden Layers**

Now, we are going to write a pipeline for the titles and text. Since `title` and `text` are different part of text but share similar vocabulary, we are going to use shared layers for input.


```python
# sharing embedding
embedding = layers.Embedding(size_vocabulary, 16, name = "embedding")

# pipeline for title
title_features = vectorize_layer(title_input)
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

# Next, we write a pipeline for the text
text_features = vectorize_layer(text_input)
text_features = embedding(text_features)
# I increased the dropout to improve overfitting
# after first time trained model2
text_features = layers.Dropout(0.75)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.5)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

# output layers for model 1 and model 2
title_output = layers.Dense(2, name = "fake")(title_features)
text_output = layers.Dense(2, name = "fake")(text_features)

# output layers for model 3
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation='relu')(main)
model3_output = layers.Dense(2, name = "fake")(main)
```

Now, it's time to create our model! In order to save our time, I've enclose the model training process in a function called `training_model`. We are able to re-use this function for our model1, model2, and model3.


```python
from matplotlib import pyplot as plt

def training_model(input, output):
  # create model
  model = Model(
      inputs = input,
      outputs = output
  )

  # Complile the model
  model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
              )
  # train the model
  history = model.fit(train_dataset, 
                      validation_data=val_dataset,
                      epochs = 50, 
                      verbose = False)
  
  # plot the training and validation accuracy
  plt.plot(history.history["accuracy"])
  plt.plot(history.history["val_accuracy"])

  # return result
  return model, history
```

**Model 1: Train Using Title**



```python
model1, history1 = training_model(title_input, title_output)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)




![png](https://github.com/jeff1hwang/jeff1hwang.github.io/blob/master/images/blog_post_6/output_37_1.png?raw=true)
    



```python
# Visualiza the structure of our model1
tf.keras.utils.plot_model(model1)
```




![png](https://github.com/jeff1hwang/jeff1hwang.github.io/blob/master/images/blog_post_6/output_38_0.png?raw=true)
    




```python
# Output the maximum validation accuracy of model1
round(max(history1.history["val_accuracy"]),5)
```




    0.98978



From the above result, we can see that our the validation accuracy of our model 1 reached around 98.6%, which is not bad! However, we can do it better in our next model!

**Model 2: Train Using Text**


```python
model2, history2 = training_model(text_input, text_output)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)




![png](https://github.com/jeff1hwang/jeff1hwang.github.io/blob/master/images/blog_post_6/output_42_1.png?raw=true)
    



```python
# Visualiza the structure of our model2
tf.keras.utils.plot_model(model2)
```




![png](https://github.com/jeff1hwang/jeff1hwang.github.io/blob/master/images/blog_post_6/output_43_0.png?raw=true)
    




```python
# Output the maximum validation accuracy of model2
round(max(history2.history["val_accuracy"]),5)
```




    0.99



Our model2 reached 99% validation accuracy, which is not bad!

**Model 3**


```python
model3, history3 = training_model([title_input, text_input], model3_output)
```


![png](https://github.com/jeff1hwang/jeff1hwang.github.io/blob/master/images/blog_post_6/output_47_0.png?raw=true)
    



```python
# Visualiza the structure of our model3
tf.keras.utils.plot_model(model3)
```




![png](https://github.com/jeff1hwang/jeff1hwang.github.io/blob/master/images/blog_post_6/output_48_0.png?raw=true)
    




```python
# Output the maximum validation accuracy of model3
round(max(history3.history["val_accuracy"]))
```




    1



By comparing the above model1, model2, model3, we conclude that model3 is the best model because this model is able to consistently obtain at least 99% accuracy on the validation set. And the best validation set reached the highest score of 100%.

## §4. Model Evaluation

In this part, we are going to test our model performance on unseen test data set. We will focus on our best model: Model 3 and ignore model1 and model2. 

Now, we first download the test data from the following URL:


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```

We would have to convert this test data using the `make_dataset` function we have created in Part 2. Then, we evaluate our best model on this data.


```python
test_data = pd.read_csv(test_url)
test_dataset = make_dataset(test_data)
```

Then, we evaluate the data


```python
model3.evaluate(test_dataset)
```

    225/225 [==============================] - 2s 9ms/step - loss: 0.0195 - accuracy: 0.9950





    [0.019546041265130043, 0.9949663877487183]



According to the above result, we can see that we are able to correctly detect and classify a fake news with 99.5% accuracy.

## §5. Embedding Visualization

In the last section, we are going to take a look at the embedding learned by our model! 


```python
# get the weights from the embedding layer
weights = model3.get_layer('embedding').get_weights()[0]

# get the vocabulary from our data prep for later
vocab = vectorize_layer.get_vocabulary()

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab,
    'x0' : weights[:,0],
    'x1' : weights[:,1]
    })
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 5,
                 hover_name = "word")

fig.show()
```

{% include post6-figure1.html %}



From the above visualization, we found that some poinits of words are useful to distinguish whether it is a fake news. For example, `trump`, `obamas`, `myanmar`, `rohingya`, `reportedly`, `gop`.
